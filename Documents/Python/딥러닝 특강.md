191106(수)

# 딥러닝 특강3

### Review

- gradient vanishing problem
  - 초기화 문제 -> Xavier 쓰면 해결
  - activation function을 sigmoid를 사용할 때 문제점이 있음 -> 값이 너무 작다. 계속 작아진다.
  - -> reLU를 쓰면 해결!
- reLU 를 activation function을 사용하면 값이 0부터 무한대까지 나옴. -> 그래서 마지막에는 softmax를 사용함 -> 확률처럼 다 더해서 1이 되게 값이 나옴!
- cross-entropy



### Linear Regression

-  직선을 만들기 위해서 두 개의 파라미터가 필요함(기울기, 절편)
  - y = ax+b
- 학습을 위해서는 **error function**을 먼저 정의해야 함
  - 점에서 거리까지의 오차
  -  mean of squared error  나 sum of squared error 사용

- 최적화는 어떻게? Gradient descent



#### ex09

```python
import numpy as np
import matplotlib.pyplot as plt

num_points = 1000
vectors_set = []

for i in range(num_points):
    x1 = np.random.normal(0.0, 0.55)
    y1 = x1 * 0.1 + 0.3 + np.random.normal(0.0, 0.03) # y축 방향으로 noise 추가
    vectors_set.append([x1, y1])

# random한 x, y 저장
x_data = [v[0] for v in vectors_set]
y_data = [v[1] for v in vectors_set]

plt.plot(x_data, y_data, 'ro')
plt.show()
```



#### ex10

```python
import tensorflow as tf

W = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) 
b = tf.Variable(tf.zeros([1])) # 0으로 초기화
y = W * x_data +b

loss = tf.reduce_mean(tf.square(y- y_data)) # reduce_mean : 차원 줄인다는 의미

optimizer = tf.train.GradientDescentOptimizer(0.5) # 0.5 : learning rate
train = optimizer.minimize(loss)

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

for step in range(10):
    print(step, sess.run(W), sess.run(b))
    sess.run(train)
    
for step in range(10):
    sess.run(train)
    print(step, sess.run(W), sess.run(b), sess.run(loss))
    #plotting
    plt.plot(x_data, y_data, 'ro')
    plt.plot(x_data, sess.run(W) * x_data + sess.run(b))
    plt.xlabel('x')
    plt.xlim(-2, 2)
    plt.ylim(0.1, 0.6)
    plt.ylabel('y')
    plt.show()
```



### batch / iteration / epoch

- batch size : 한번에 몇 개의 데이터를 누적해서 처리할건지 (한번에 처리하는 데이터 갯수)
- iteration : 업데이트 횟수
- epoch : 데이터셋을 한 번 다 훑은 횟수



Graident descent optimizer와 Adam optimizer 중엔 Adam이 더 좋음

